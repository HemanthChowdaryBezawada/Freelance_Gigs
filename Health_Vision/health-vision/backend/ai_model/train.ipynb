{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HealthVision: AI Fall Detection Training\n",
                "\n",
                "This notebook trains a Hybrid LSTM model on pose keypoints extracted from video to detect falls.\n",
                "**Architecture**: Input (30 frames, 34 keypoints) -> LSTM/Transformer -> Class Probability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install kagglehub ultralytics\n",
                "import kagglehub\n",
                "import os\n",
                "import shutil\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.utils import class_weight\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Import our local modules\n",
                "from model import build_fall_detection_model, build_transformer_model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Download & Organize Datasets\n",
                "We will auto-download the datasets and organize them into a standard structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Downloading datasets...\")\n",
                "ur_fall_path = kagglehub.dataset_download(\"shahliza27/ur-fall-detection-dataset\")\n",
                "\n",
                "print(\"UR Fall Path:\", ur_fall_path)\n",
                "\n",
                "# Setup Directories\n",
                "base_dir = \"dataset_organized\"\n",
                "if os.path.exists(base_dir):\n",
                "    shutil.rmtree(base_dir)\n",
                "os.makedirs(f\"{base_dir}/fall\", exist_ok=True)\n",
                "os.makedirs(f\"{base_dir}/normal\", exist_ok=True)\n",
                "\n",
                "def organize_dataset_leaves(source_dir, dest_base):\n",
                "    count_fall = 0\n",
                "    count_normal = 0\n",
                "    \n",
                "    # Walk deeply\n",
                "    for root, dirs, files in os.walk(source_dir):\n",
                "        # Check if LEAF node (has images)\n",
                "        has_images = any(f.lower().endswith(('.png', '.jpg')) for f in files)\n",
                "        \n",
                "        if has_images:\n",
                "            dirname = os.path.basename(root)\n",
                "            lower = dirname.lower()\n",
                "            \n",
                "            target_class = None\n",
                "            if 'adl' in lower or 'd0' in lower:\n",
                "                target_class = 'normal'\n",
                "            elif 'fall' in lower or 'f0' in lower:\n",
                "                target_class = 'fall'\n",
                "            \n",
                "            if target_class:\n",
                "                src = root\n",
                "                dst = os.path.join(dest_base, target_class, dirname)\n",
                "                # Copy this leaf folder\n",
                "                if not os.path.exists(dst):\n",
                "                    shutil.copytree(src, dst)\n",
                "                    if target_class == 'normal': count_normal += 1\n",
                "                    else: count_fall += 1\n",
                "                    \n",
                "    return count_fall, count_normal\n",
                "\n",
                "print(\"Organizing files...\")\n",
                "f, n = organize_dataset_leaves(ur_fall_path, base_dir)\n",
                "print(f\"Organized: {f} Fall sequences, {n} Normal sequences.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing\n",
                "Extract keypoints."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python preprocess.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed data\n",
                "if os.path.exists('processed_data.npy'):\n",
                "    data = np.load('processed_data.npy', allow_pickle=True).item()\n",
                "    X = data['X']\n",
                "    y = data['y']\n",
                "\n",
                "    print(f\"Total Samples: {X.shape[0]}\")\n",
                "    print(f\"Feature Shape: {X.shape[1:]}\") \n",
                "    \n",
                "    unique_classes = np.unique(y)\n",
                "    print(f\"Classes found: {unique_classes}\")\n",
                "    num_classes = 3 # Force 3 classes architecture\n",
                "\n",
                "    y_encoded = keras.utils.to_categorical(y, num_classes=num_classes)\n",
                "\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "    print(f\"Training samples: {len(X_train)}\")\n",
                "    print(f\"Testing samples: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'X_train' in locals() and len(X_train) > 0:\n",
                "    input_shape = (30, 34)\n",
                "    model = build_fall_detection_model(input_shape, num_classes)\n",
                "    model.summary()\n",
                "\n",
                "    # Compute weights\n",
                "    y_integers = np.argmax(y_encoded, axis=1)\n",
                "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_integers), y=y_integers)\n",
                "    class_weight_dict = dict(zip(np.unique(y_integers), class_weights))\n",
                "    print(\"Class Weights:\", class_weight_dict)\n",
                "\n",
                "    history = model.fit(\n",
                "        X_train, y_train,\n",
                "        validation_data=(X_test, y_test),\n",
                "        epochs=50,\n",
                "        batch_size=32,\n",
                "        # class_weight=class_weight_dict, # Optional, use if imbalance is huge\n",
                "        callbacks=[\n",
                "            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
                "            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
                "        ]\n",
                "    )\n",
                "else:\n",
                "    print(\"Skipping training, no data loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation & Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'model' in locals() and 'X_test' in locals():\n",
                "    loss, acc = model.evaluate(X_test, y_test)\n",
                "    print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
                "\n",
                "    # Export to TFLite with SELECT_TF_OPS for LSTM support\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    \n",
                "    # CRITICAL FIX for LSTM layers conversion:\n",
                "    # Enable standard TFLite ops and specific TensorFlow ops (Select TF Ops)\n",
                "    converter.target_spec.supported_ops = [\n",
                "        tf.lite.OpsSet.TFLITE_BUILTINS, \n",
                "        tf.lite.OpsSet.SELECT_TF_OPS\n",
                "    ]\n",
                "    # Disable experimental LOWERING of Tensor List ops which causes the 'legalize' error\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite_model = converter.convert()\n",
                "\n",
                "    with open('fall_detection_model.tflite', 'wb') as f:\n",
                "        f.write(tflite_model)\n",
                "        \n",
                "    print(\"SUCCESS: Model saved as 'fall_detection_model.tflite'\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}